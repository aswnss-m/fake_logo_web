{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_name = \"feature_vectors_\"\n",
    "csv_name = \"annotated_feature_vectors_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory path\n",
    "directory = 'C:/Users/ACER/Documents/GitHub/fake-logo-web/backend/src/'\n",
    "\n",
    "# Get the list of npy files\n",
    "npy_files = glob.glob(directory + 'feature_vectors_*.npy')\n",
    "\n",
    "# Get the list of csv files\n",
    "csv_files = glob.glob(directory + 'annotated_feature_vectors_*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Sort the files\n",
    "npy_files.sort()\n",
    "csv_files.sort()\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 3\n",
    "\n",
    "# Iterate through the files in batches\n",
    "for i in range(0, len(npy_files), batch_size):\n",
    "    # Get the batch of npy files\n",
    "    npy_batch = npy_files[i:i+batch_size]\n",
    "    \n",
    "    # Get the batch of csv files\n",
    "    csv_batch = csv_files[i:i+batch_size]\n",
    "    \n",
    "    # Load the npy files\n",
    "    feature_vectors = [np.load(file) for file in npy_batch]\n",
    "    \n",
    "    # Combine the npy files\n",
    "    combined_features = np.concatenate(feature_vectors, axis=0)\n",
    "    \n",
    "    # Save the combined npy file\n",
    "    np.save(directory + f'combined_features_{i}.npy', combined_features)\n",
    "    \n",
    "    # Load the csv files\n",
    "    dfs = [pd.read_csv(file) for file in csv_batch]\n",
    "    \n",
    "    # Combine the csv files\n",
    "    combined_csv = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Save the combined csv file\n",
    "    combined_csv.to_csv(directory + f'combined_annotations_{i}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy = [np.load(file) for file in [\"combined_features_0.npy\", \"combined_features_3.npy\", \"combined_features_6.npy\",\"combined_features_9.npy\",\"combined_features_12.npy\"]]\n",
    "df = [pd.read_csv(file) for file in [\"combined_annotations_0.csv\", \"combined_annotations_3.csv\", \"combined_annotations_6.csv\",\"combined_annotations_9.csv\",\"combined_annotations_12.csv\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\Documents\\GitHub\\fake-logo-web\\backend\\.env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(file)\n\u001b[0;32m     15\u001b[0m     kmeans \u001b[39m=\u001b[39m KMeans(n_clusters\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m     clusters \u001b[39m=\u001b[39m kmeans\u001b[39m.\u001b[39;49mfit_predict(data)\n\u001b[0;32m     17\u001b[0m     npy_clusters\u001b[39m.\u001b[39mappend(clusters)\n\u001b[0;32m     19\u001b[0m \u001b[39m# Combine the npy clusters\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Documents\\GitHub\\fake-logo-web\\backend\\.env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1033\u001b[0m, in \u001b[0;36m_BaseKMeans.fit_predict\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_predict\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1011\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute cluster centers and predict cluster index for each sample.\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m \n\u001b[0;32m   1013\u001b[0m \u001b[39m    Convenience method; equivalent to calling fit(X) followed by\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[39m        Index of the cluster each sample belongs to.\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1033\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, sample_weight\u001b[39m=\u001b[39;49msample_weight)\u001b[39m.\u001b[39mlabels_\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Documents\\GitHub\\fake-logo-web\\backend\\.env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1461\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1457\u001b[0m best_inertia, best_labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1459\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_init):\n\u001b[0;32m   1460\u001b[0m     \u001b[39m# Initialize centers\u001b[39;00m\n\u001b[1;32m-> 1461\u001b[0m     centers_init \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_centroids(\n\u001b[0;32m   1462\u001b[0m         X, x_squared_norms\u001b[39m=\u001b[39;49mx_squared_norms, init\u001b[39m=\u001b[39;49minit, random_state\u001b[39m=\u001b[39;49mrandom_state\n\u001b[0;32m   1463\u001b[0m     )\n\u001b[0;32m   1464\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n\u001b[0;32m   1465\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInitialization complete\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Documents\\GitHub\\fake-logo-web\\backend\\.env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:989\u001b[0m, in \u001b[0;36m_BaseKMeans._init_centroids\u001b[1;34m(self, X, x_squared_norms, init, random_state, init_size, n_centroids)\u001b[0m\n\u001b[0;32m    986\u001b[0m     n_samples \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    988\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(init, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m init \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mk-means++\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 989\u001b[0m     centers, _ \u001b[39m=\u001b[39m _kmeans_plusplus(\n\u001b[0;32m    990\u001b[0m         X,\n\u001b[0;32m    991\u001b[0m         n_clusters,\n\u001b[0;32m    992\u001b[0m         random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[0;32m    993\u001b[0m         x_squared_norms\u001b[39m=\u001b[39;49mx_squared_norms,\n\u001b[0;32m    994\u001b[0m     )\n\u001b[0;32m    995\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(init, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m init \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrandom\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    996\u001b[0m     seeds \u001b[39m=\u001b[39m random_state\u001b[39m.\u001b[39mpermutation(n_samples)[:n_clusters]\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Documents\\GitHub\\fake-logo-web\\backend\\.env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:234\u001b[0m, in \u001b[0;36m_kmeans_plusplus\u001b[1;34m(X, n_clusters, x_squared_norms, random_state, n_local_trials)\u001b[0m\n\u001b[0;32m    231\u001b[0m np\u001b[39m.\u001b[39mclip(candidate_ids, \u001b[39mNone\u001b[39;00m, closest_dist_sq\u001b[39m.\u001b[39msize \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, out\u001b[39m=\u001b[39mcandidate_ids)\n\u001b[0;32m    233\u001b[0m \u001b[39m# Compute distances to center candidates\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m distance_to_candidates \u001b[39m=\u001b[39m _euclidean_distances(\n\u001b[0;32m    235\u001b[0m     X[candidate_ids], X, Y_norm_squared\u001b[39m=\u001b[39;49mx_squared_norms, squared\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    236\u001b[0m )\n\u001b[0;32m    238\u001b[0m \u001b[39m# update closest distances squared and potential for each candidate\u001b[39;00m\n\u001b[0;32m    239\u001b[0m np\u001b[39m.\u001b[39mminimum(closest_dist_sq, distance_to_candidates, out\u001b[39m=\u001b[39mdistance_to_candidates)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Documents\\GitHub\\fake-logo-web\\backend\\.env\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:366\u001b[0m, in \u001b[0;36m_euclidean_distances\u001b[1;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[0;32m    361\u001b[0m         YY \u001b[39m=\u001b[39m row_norms(Y, squared\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[np\u001b[39m.\u001b[39mnewaxis, :]\n\u001b[0;32m    363\u001b[0m \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mfloat32:\n\u001b[0;32m    364\u001b[0m     \u001b[39m# To minimize precision issues with float32, we compute the distance\u001b[39;00m\n\u001b[0;32m    365\u001b[0m     \u001b[39m# matrix on chunks of X and Y upcast to float64\u001b[39;00m\n\u001b[1;32m--> 366\u001b[0m     distances \u001b[39m=\u001b[39m _euclidean_distances_upcast(X, XX, Y, YY)\n\u001b[0;32m    367\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    368\u001b[0m     \u001b[39m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[0;32m    369\u001b[0m     distances \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m safe_sparse_dot(X, Y\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Documents\\GitHub\\fake-logo-web\\backend\\.env\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:562\u001b[0m, in \u001b[0;36m_euclidean_distances_upcast\u001b[1;34m(X, XX, Y, YY, batch_size)\u001b[0m\n\u001b[0;32m    559\u001b[0m     d \u001b[39m=\u001b[39m distances[y_slice, x_slice]\u001b[39m.\u001b[39mT\n\u001b[0;32m    561\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 562\u001b[0m     Y_chunk \u001b[39m=\u001b[39m Y[y_slice]\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49mfloat64)\n\u001b[0;32m    563\u001b[0m     \u001b[39mif\u001b[39;00m YY \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    564\u001b[0m         YY_chunk \u001b[39m=\u001b[39m row_norms(Y_chunk, squared\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[np\u001b[39m.\u001b[39mnewaxis, :]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# List of npy files\n",
    "npy_files = [\"combined_features_0.npy\", \"combined_features_3.npy\", \"combined_features_6.npy\", \"combined_features_9.npy\", \"combined_features_12.npy\"]\n",
    "\n",
    "# List of csv files\n",
    "csv_files = [\"combined_annotations_0.csv\", \"combined_annotations_3.csv\", \"combined_annotations_6.csv\", \"combined_annotations_9.csv\", \"combined_annotations_12.csv\"]\n",
    "\n",
    "# Perform k-means clustering on each npy file separately\n",
    "npy_clusters = []\n",
    "for file in npy_files:\n",
    "    data = np.load(file)\n",
    "    kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "    clusters = kmeans.fit_predict(data)\n",
    "    npy_clusters.append(clusters)\n",
    "\n",
    "# Combine the npy clusters\n",
    "combined_npy_clusters = np.concatenate(npy_clusters)\n",
    "\n",
    "# Save the combined npy clusters to a file\n",
    "np.save(\"combined_npy_clusters.npy\", combined_npy_clusters)\n",
    "\n",
    "# Perform k-means clustering on each csv file separately\n",
    "csv_clusters = []\n",
    "for file in csv_files:\n",
    "    data = pd.read_csv(file)\n",
    "    kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "    clusters = kmeans.fit_predict(data)\n",
    "    csv_clusters.append(clusters)\n",
    "\n",
    "# Combine the csv clusters\n",
    "combined_csv_clusters = np.concatenate(csv_clusters)\n",
    "\n",
    "# Save the combined csv clusters to a file\n",
    "np.save(\"combined_csv_clusters.npy\", combined_csv_clusters)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m file_path, save_path \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(file_paths, save_paths):\n\u001b[0;32m      8\u001b[0m     npy_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(file_path)\n\u001b[1;32m----> 9\u001b[0m     np\u001b[39m.\u001b[39;49msavetxt(save_path, npy_array, delimiter\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msavetxt\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Documents\\GitHub\\fake-logo-web\\backend\\.env\\lib\\site-packages\\numpy\\lib\\npyio.py:1595\u001b[0m, in \u001b[0;36msavetxt\u001b[1;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[0;32m   1591\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1592\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMismatch between array dtype (\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m) and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1593\u001b[0m                             \u001b[39m\"\u001b[39m\u001b[39mformat specifier (\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1594\u001b[0m                             \u001b[39m%\u001b[39m (\u001b[39mstr\u001b[39m(X\u001b[39m.\u001b[39mdtype), \u001b[39mformat\u001b[39m)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m-> 1595\u001b[0m         fh\u001b[39m.\u001b[39;49mwrite(v)\n\u001b[0;32m   1597\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(footer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1598\u001b[0m     footer \u001b[39m=\u001b[39m footer\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m comments)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "file_paths = [\"combined_features_0.npy\", \"combined_features_3.npy\", \"combined_features_6.npy\", \"combined_features_9.npy\", \"combined_features_12.npy\"]\n",
    "save_paths = [\"reduced_features_0.txt\", \"reduced_features_3.txt\", \"reduced_features_6.txt\", \"reduced_features_9.txt\", \"reduced_features_12.txt\"]\n",
    "\n",
    "for file_path, save_path in zip(file_paths, save_paths):\n",
    "    npy_array = np.load(file_path)\n",
    "    np.savetxt(save_path, npy_array, delimiter=',')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
